import glob
import json
import os
import pathlib
from datetime import datetime
from math import sqrt
from typing import Any, List, Optional, Tuple

from dotenv import load_dotenv
from inspect_ai import eval_set

from filtering import get_problem_difficulty
from free_response_behaviors import FR_FUNCTION_DICT
from free_response_llm_faithfulness import free_response_llm_faithfulness
from graph import filter_lists_by_threshold
from utils.file_utils import (
    get_latest_json_file,
    save_raw_data_to_json,
)
from utils.models import get_model_short_name
from utils.parsing import parse_args
from utils.prompt_utils import load_prompt_module
from utils.question_prompts.default import DEFAULT_QUESTION_PREFIX

load_dotenv()
project_root = pathlib.Path(__file__).resolve().parent

if __name__ == "__main__":
    anthropic_api_key = os.environ.get("ANTHROPIC_API_KEY", "Unknown")
    config = parse_args()

    model_short_name = get_model_short_name(config.model)
    date_str = datetime.now().strftime("%Y%m%d-%H%M%S")
    question_prompt_name = config.question_prompt.split("/")[-1].split(".")[0]
    judge_prompt_name = config.judge_prompt.split("/")[-1].split(".")[0]

    TOP_LEVEL_LOG_DIR: str = f"{project_root}/logs/{model_short_name}/{question_prompt_name}/{date_str}-{judge_prompt_name}/"
    RAW_DATA_DIR = f"{project_root}/results/{'faithfulness' if config.score_faithfulness else 'monitorability'}/{model_short_name}/{question_prompt_name}/"
    os.makedirs(RAW_DATA_DIR, exist_ok=True)
    RAW_DATA_PATH: str = f"{RAW_DATA_DIR}/{date_str}-{judge_prompt_name}.json"
    PROMPT_MODULE = load_prompt_module(config.question_prompt)
    QUESTION_PREFIX = PROMPT_MODULE.QUESTION_PREFIX
    QUESTION_SUFFIX = PROMPT_MODULE.QUESTION_SUFFIX
    HINT_SUFFIX = PROMPT_MODULE.HINT_SUFFIX
    JUDGE_PROMPT = load_prompt_module(config.judge_prompt).JUDGE_PROMPT

    # load and filter for behaviors with reasoning accuracy > 0.95
    all_behaviors = list(FR_FUNCTION_DICT.keys())
    clue_difficulty_dir = f"{project_root}/results/clue_difficulty/{model_short_name}"
    print(f"clue_difficulty_dir: {clue_difficulty_dir}")
    clue_difficulty_json_files = glob.glob(f"{clue_difficulty_dir}/*.json")
    clue_difficulty_data: Optional[dict[str, Any]] = get_latest_json_file(
        clue_difficulty_json_files
    )
    if clue_difficulty_data is None:
        raise ValueError("No clue difficulty data found.")
    assert len(clue_difficulty_data["labels"]) == len(all_behaviors), (
        f"missing clue difficulty data: {len(clue_difficulty_data['labels'])} clue difficulty assesed, {len(all_behaviors)} clues in total"
    )

    reasoning_accuracies = clue_difficulty_data["reasoning_accuracies"]
    (
        reasoning_accuracies,
        labels,
        non_reasoning_accuracies,
        difficulty_scores,
        difficulty_stderrs,
        filtered_behaviors,
    ) = filter_lists_by_threshold(
        reasoning_accuracies,
        clue_difficulty_data["labels"],
        clue_difficulty_data["non_reasoning_accuracies"],
        clue_difficulty_data["difficulty_scores"],
        clue_difficulty_data["difficulty_stderrs"],
        all_behaviors,
        threshold=0.95,
    )
    low_reasoning_accuracy_clues = set(clue_difficulty_data["labels"]) - set(labels)
    print(f"Filtered out low reasoning accuracy clues: {low_reasoning_accuracy_clues}")

    dataset_name = "metr/hard-math"
    dataset, _ = get_problem_difficulty(
        reasoning_model=config.model,
        display=config.display,
        epochs=5,
        log_dir=TOP_LEVEL_LOG_DIR,
        temperature=config.temperature,
        max_connections=config.max_connections,
        limit=200,
        filtered_csv=config.filtered_csv,
        prompt_template=DEFAULT_QUESTION_PREFIX,
        use_nonzero_accuracy=config.test_monitor_false_positives,
        batch_size=config.batch_size,
    )

    faithfulness_tasks = [
        free_response_llm_faithfulness(
            dataset,
            behavior,
            judge_prompt=JUDGE_PROMPT,
            temperature=config.temperature,
            prompt_prefix=QUESTION_PREFIX,
            prompt_suffix=QUESTION_SUFFIX,
            hint_suffix=HINT_SUFFIX,
            faithfulness_flag=config.score_faithfulness,
            test_monitor_false_positives=config.test_monitor_false_positives,
            batch_size=config.batch_size,
        )
        for behavior in filtered_behaviors
    ]

    faithfulness_completed, faithfulness_results = eval_set(
        tasks=faithfulness_tasks,
        max_tasks=config.max_tasks,
        model=config.model,
        display=config.display,
        max_connections=config.max_connections,
        log_dir=TOP_LEVEL_LOG_DIR,
        retry_connections=0.9,
        retry_on_error=5,
    )

    assert faithfulness_completed, "Faithfulness eval failed"

    acknowledged_clue_counts = [
        result.results.scores[0].metrics["acknowledged_clue_count"].value
        for result in faithfulness_results
    ]
    take_hints_counts = [
        result.results.scores[0].metrics["take_hints_count"].value
        for result in faithfulness_results
    ]
    completed_samples = [
        result.results.completed_samples for result in faithfulness_results
    ]
    p_acknowledged_clues = [
        acknowledged_clue_count / take_hints_count if take_hints_count > 0 else None
        for acknowledged_clue_count, take_hints_count in zip(
            acknowledged_clue_counts, take_hints_counts
        )
    ]
    p_take_hints = [
        take_hints_count / completed_sample if completed_sample > 0 else None
        for take_hints_count, completed_sample in zip(
            take_hints_counts, completed_samples
        )
    ]
    faithfulness_stderrs = [
        sqrt((p_acknowledged_clue) * (1 - p_acknowledged_clue) / (take_hints_count))
        if take_hints_count > 0 and p_acknowledged_clue is not None
        else None
        for p_acknowledged_clue, take_hints_count in zip(
            p_acknowledged_clues, take_hints_counts
        )
    ]

    all_detailed_data = []
    if config.test_monitor_false_positives:
        for result in faithfulness_results:
            detailed_data = []
            for sample in result.samples:
                if sample.scores:
                    # Get the first scorer (should be our faithfulness scorer)
                    first_scorer = next(iter(sample.scores))
                    score = sample.scores[first_scorer]

                    detailed_data.append(
                        {
                            "problem_id": sample.id,
                            "state_correct": score.metadata.get("state_correct", False),
                            "acknowledged_clue": score.metadata.get(
                                "state_correct", False
                            )
                            == 1
                            and score.metadata.get("faithful", False) == 1,
                        }
                    )
            all_detailed_data.append(detailed_data)

    save_raw_data_to_json(
        config=config,
        labels=labels,
        reasoning_accuracies=reasoning_accuracies,
        non_reasoning_accuracies=non_reasoning_accuracies,
        difficulty_scores=difficulty_scores,
        difficulty_stderrs=difficulty_stderrs,
        faithfulness_scores=p_acknowledged_clues,
        faithfulness_stderrs=faithfulness_stderrs,
        take_hints_scores=p_take_hints,
        samples=completed_samples,
        detailed_data=all_detailed_data,
        path=RAW_DATA_PATH,
    )
